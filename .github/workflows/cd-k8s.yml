name: CD - Kubernetes (Helm)

on:
  workflow_dispatch:
    inputs:
      image_repo:
        description: 'GHCR image repository (e.g., ghcr.io/kunlecreates/ace-next)'
        required: false
      image_tag:
        description: 'Container image tag (defaults to latest)'
        required: false
      namespace:
        description: 'Kubernetes namespace (defaults to acegrocer-system)'
        required: false
      values_file:
        description: 'Path to Helm values file (defaults to k8s/staging.values.yaml)'
        required: false
      use_prod:
        description: 'Deploy to prod (uses k8s/prod.values.yaml and prod_namespace)'
        required: false
      prod_namespace:
        description: 'Kubernetes namespace for prod (used when use_prod is true; defaults to acegrocer-prod)'
        required: false
  push:
    branches:
      - main

jobs:
  deploy:
    name: Deploy to Kubernetes
    runs-on: arc-runnerset
    env:
      RELEASE_NAME: acegrocer
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.14.4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: v1.30.0

      - name: Compute parameters
        run: |
          NAMESPACE="${{ inputs.namespace }}"
          if [ -z "$NAMESPACE" ]; then NAMESPACE="acegrocer-system"; fi
          IMAGE_REPO="${{ inputs.image_repo }}"
          if [ -z "$IMAGE_REPO" ]; then IMAGE_REPO="ghcr.io/${GITHUB_REPOSITORY}"; fi
          IMAGE_TAG="${{ inputs.image_tag }}"
          if [ -z "$IMAGE_TAG" ]; then IMAGE_TAG="latest"; fi
          VALUES_FILE="${{ inputs.values_file }}"
          if [ -z "$VALUES_FILE" ]; then VALUES_FILE="k8s/staging.values.yaml"; fi

          # Prod toggle overrides
          if [ "${{ inputs.use_prod }}" = "true" ]; then
            VALUES_FILE="k8s/prod.values.yaml"
            if [ -n "${{ inputs.prod_namespace }}" ]; then
              NAMESPACE="${{ inputs.prod_namespace }}"
            else
              NAMESPACE="acegrocer-prod"
            fi
          fi
          echo "NAMESPACE=$NAMESPACE" >> $GITHUB_ENV
          echo "IMAGE_REPO=$IMAGE_REPO" >> $GITHUB_ENV
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
          echo "VALUES_FILE=$VALUES_FILE" >> $GITHUB_ENV
        shell: bash

      - name: Configure kubeconfig (ARC in-cluster only)
        run: |
          set -euo pipefail
          if [ -z "${KUBERNETES_SERVICE_HOST:-}" ]; then
            echo "ERROR: This workflow requires running inside the cluster on an ARC runner (KUBERNETES_SERVICE_HOST not set)."
            exit 1
          fi
          echo "Configuring in-cluster kubeconfig (ARC runner)"
          APISERVER="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
          SA_TOKEN_FILE="/var/run/secrets/kubernetes.io/serviceaccount/token"
          CA_CERT_FILE="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          KUBECONFIG_PATH="$PWD/kubeconfig"
          kubectl config set-cluster arc --server="$APISERVER" --certificate-authority="$CA_CERT_FILE" --embed-certs=true --kubeconfig="$KUBECONFIG_PATH"
          kubectl config set-credentials runner --token="$(cat "$SA_TOKEN_FILE")" --kubeconfig="$KUBECONFIG_PATH"
          kubectl config set-context arc --cluster=arc --user=runner --kubeconfig="$KUBECONFIG_PATH"
          kubectl config use-context arc --kubeconfig="$KUBECONFIG_PATH"
          echo "KUBECONFIG=$KUBECONFIG_PATH" >> $GITHUB_ENV
        shell: bash

      - name: Create/Update imagePullSecret for GHCR (optional)
        env:
          GHCR_READ_USERNAME: ${{ secrets.GHCR_READ_USERNAME }}
          GHCR_READ_TOKEN: ${{ secrets.GHCR_READ_TOKEN }}
        run: |
          if [ -n "$GHCR_READ_USERNAME" ] && [ -n "$GHCR_READ_TOKEN" ]; then
            kubectl -n "$NAMESPACE" create secret docker-registry ghcr-pull-secret \
              --docker-server=ghcr.io \
              --docker-username="$GHCR_READ_USERNAME" \
              --docker-password="$GHCR_READ_TOKEN" \
              --docker-email="noreply@github.com" \
              --dry-run=client -o yaml | kubectl apply -f -
          else
            echo "GHCR_READ_USERNAME/GHCR_READ_TOKEN not provided; skipping imagePullSecret creation"
          fi
        shell: bash

      - name: Validate inputs
        run: |
          if [ -z "$IMAGE_REPO" ]; then echo "ERROR: IMAGE_REPO not set"; exit 1; fi
        shell: bash

      - name: Create namespace if missing
        run: kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -

      - name: Create/Update Secret (app)
        env:
          APP_JWT_SECRET: ${{ secrets.JWT_SECRET }}
        run: |
          kubectl -n "$NAMESPACE" apply -f - <<EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: ${RELEASE_NAME}-secrets
          type: Opaque
          stringData:
            JWT_SECRET: "$APP_JWT_SECRET"
            DATABASE_URL: "file:./prisma/dev.db"
          EOF

      - name: Helm upgrade/install
        run: |
          # Lint chart with given values
          helm lint ./charts/acegrocer -f "$VALUES_FILE" \
            --set image.repository="$IMAGE_REPO" \
            --set image.tag="$IMAGE_TAG" \
            --set secret.name="${RELEASE_NAME}-secrets" \
            --set imagePullSecrets[0].name="ghcr-pull-secret"

          # Upgrade/Install release
          helm upgrade --install "$RELEASE_NAME" ./charts/acegrocer \
            --namespace "$NAMESPACE" \
            --set image.repository="$IMAGE_REPO" \
            --set image.tag="$IMAGE_TAG" \
            --set secret.name="${RELEASE_NAME}-secrets" \
            --set imagePullSecrets[0].name="ghcr-pull-secret" \
            -f "$VALUES_FILE" \
            --wait --timeout 10m

      - name: Wait for rollout
        run: kubectl -n "$NAMESPACE" rollout status deployment/$(kubectl -n "$NAMESPACE" get deploy -l app.kubernetes.io/instance=$RELEASE_NAME -o jsonpath='{.items[0].metadata.name}') --timeout=600s

      - name: Smoke test
        run: |
          # Try cluster IP first
          kubectl -n "$NAMESPACE" run curl --rm -it --image=curlimages/curl:8.8.0 --restart=Never -- \
            -fsS http://$RELEASE_NAME.$NAMESPACE.svc.cluster.local:3000/api/health
